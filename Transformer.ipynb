{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhijeet/miniconda3/envs/TRIZ/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_epochs = 500\n",
    "model_no = 'transformer_with_word2vec'\n",
    "exp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, no_out_vect = 128, num_classes=128, hidden_dim=100, nheads=10, num_encoder_layers=5, \n",
    "                 num_decoder_layers=5):\n",
    "        super(custom_transformer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         self.transformer = nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers, \n",
    "#                                           batch_first=True, activation=\"relu\")\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder, num_layers = num_encoder_layers)\n",
    "        \n",
    "        self.decoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads)\n",
    "        self.transformer_decoder = nn.TransformerEncoder(self.encoder, num_layers = num_encoder_layers)\n",
    "        \n",
    "        self.relu_layer = nn.ReLU()\n",
    "        self.sigmoid_layer = nn.Sigmoid()\n",
    "        \n",
    "    def positionalencoding1d(self, d_model, length):\n",
    "        \"\"\"\n",
    "        :param d_model: dimension of the model\n",
    "        :param length: length of positions\n",
    "        :return: length*d_model position matrix\n",
    "        \"\"\"\n",
    "        if d_model % 2 != 0:\n",
    "            raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                             \"odd dim (got dim={:d})\".format(d_model))\n",
    "        pe = torch.zeros(length, d_model)\n",
    "        position = torch.arange(0, length).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "                             -(math.log(10000.0) / d_model)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "\n",
    "        return pe\n",
    "\n",
    "    def forward(self, feat_input):\n",
    "        feat_input = feat_input.flatten(2)\n",
    "\n",
    "        feat_input += self.positionalencoding1d(self.hidden_dim, feat_input.shape[-2]).repeat(feat_input.shape[0], feat_input.shape[1], feat_input.shape[2])\n",
    "#         features = self.transformer(feat_input.cuda(), self.learnable_query.repeat(feat_input.shape[0], 1, 1))\n",
    "        enc_features = self.transformer_encoder(feat_input)\n",
    "        dec_features = self.transformer_decoder(feat_input, enc_features)\n",
    "        features = self.linear1(features.flatten(1))\n",
    "        features = self.sigmoid_layer(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params: 5430576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhijeet/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "new_model = custom_transformer()\n",
    "# new_model.to(device)\n",
    "print(\"Total trainable params:\", torch.nn.utils.parameters_to_vector([p for p in new_model.parameters() if p.requires_grad]).numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_transformer(\n",
       "  (encoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-4): 5 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_decoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-4): 5 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (relu_layer): ReLU()\n",
       "  (sigmoid_layer): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class dataset_loader(Dataset):\n",
    "    \n",
    "    def __init__(self, corpus_dir, word2vec_dir):\n",
    "        self.corpus_dir = corpus_dir\n",
    "        self.word2vec_all = open('/home/abhijeet/Desktop/TRIZ/word_vectors.json', 'r')\n",
    "        self.word2vec_all = json.load(self.word2vec_all)\n",
    "        self.dataset = pd.read_excel(self.corpus_dir + 'generated_data.xlsx')\n",
    "        self.dataset = self.dataset.dropna()\n",
    "        self.text = self.dataset['text']\n",
    "        self.label = self.dataset['labels']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.text.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row_text = self.text[idx].lower().replace('\\n', ' ').split()\n",
    "        row_label = eval(self.label[idx])\n",
    "#         print(row_label.shape, row_text.shape)\n",
    "        word2vec_matrix = []\n",
    "        \n",
    "        for count, i in enumerate(row_text):\n",
    "            try:\n",
    "                word2vec_matrix.append(np.array(self.word2vec_all[i]))\n",
    "                if count == 20000:\n",
    "                    break\n",
    "            except:\n",
    "                word2vec_matrix.append(np.zeros(100))\n",
    "        \n",
    "        for i in range(20000 - len(word2vec_matrix)):\n",
    "            word2vec_matrix.append(np.zeros(100))\n",
    "            \n",
    "#         print(len(word2vec_matrix))\n",
    "\n",
    "        for next_count, i in enumerate(row_label):\n",
    "            try:\n",
    "                word2vec_matrix.append(np.array(self.word2vec_all[i]))\n",
    "                if next_count == 499:\n",
    "                    break\n",
    "            except:\n",
    "                word2vec_matrix.append(np.zeros(100))\n",
    "            \n",
    "        for i in range(499 - next_count):\n",
    "            word2vec_matrix.append(np.zeros(100))\n",
    "\n",
    "#         print(len(word2vec_matrix))\n",
    "        \n",
    "        output = {'text_label': np.array(word2vec_matrix)}\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset_loader(corpus_dir='/home/abhijeet/Desktop/TRIZ/All_data/CPC Data/',\n",
    "                              word2vec_dir='/home/abhijeet/Desktop/TRIZ/word_vectors.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(train_data))\n",
    "for i in range(len(train_data)):\n",
    "    sample = train_data[i]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "\n",
    "dataloader_train = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def criterion(predicted, target):\n",
    "    \"\"\"\n",
    "    Compute the Kullback-Leibler Divergence loss between two probability distributions.\n",
    "\n",
    "    Args:\n",
    "        p (torch.Tensor): True distribution (e.g., ground truth probabilities).\n",
    "        q (torch.Tensor): Approximate distribution (e.g., predicted probabilities).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: KL Divergence loss.\n",
    "    \"\"\"\n",
    "    return F.kl_div(F.log_softmax(target, dim=1), F.softmax(predicted, dim=1), reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.to(device)\n",
    "optimizer = torch.optim.SGD(new_model.parameters(), lr=0.009, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                                    | 0/679 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 20500, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 4303360000000000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 13\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mnew_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predicted\u001b[38;5;241m=\u001b[39mpreds, target\u001b[38;5;241m=\u001b[39my_ground_truth)\n\u001b[1;32m     17\u001b[0m inner_pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m, in \u001b[0;36mcustom_transformer.forward\u001b[0;34m(self, feat_input)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, feat_input):\n\u001b[1;32m     37\u001b[0m         feat_input \u001b[38;5;241m=\u001b[39m feat_input\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m         feat_input \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositionalencoding1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#         features = self.transformer(feat_input.cuda(), self.learnable_query.repeat(feat_input.shape[0], 1, 1))\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         enc_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(feat_input)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 4303360000000000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    num_nans = 0\n",
    "    running_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    print(f\"\\nEpoch: {epoch+1}\")\n",
    "    inner_pbar = tqdm(total=len(dataloader_train), position=1, leave=False, ascii=True, desc=f\"Epoch: {epoch+1} Step\")\n",
    "\n",
    "    for i, data in enumerate(dataloader_train, 1):\n",
    "        features = data['text_label']\n",
    "        y_ground_truth = features\n",
    "        optimizer.zero_grad()\n",
    "        print(features.shape)\n",
    "        preds = new_model(features)\n",
    "        \n",
    "        loss = criterion(predicted=preds, target=y_ground_truth)\n",
    "\n",
    "        inner_pbar.update(1)\n",
    "        \n",
    "        running_loss += loss.detach().item()\n",
    "\n",
    "        print(f\"\\rStep: {i} Training Loss: {loss.item()} Validation Loss: {val_loss} Epoch loss: {running_loss/(i)}\", end=\"\")\n",
    "        # print(f\"\\rStep: {i} Training Loss: {loss.item()} Validation Loss: {val_loss} Nans: {num_nans}\")\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            if num_nans > 10:\n",
    "                raise RuntimeError(f\"Model Error: Encountered {num_nans} nan loss\")\n",
    "            num_nans += 1\n",
    "            continue\n",
    "        # loss.requires_grad = True\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    inner_pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "original_tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# Repeat the tensor along specified dimensions\n",
    "repeated_tensor = original_tensor.repeat(2, 3)  # Repeat along dimension 0, 2 times, and along dimension 1, 3 times\n",
    "\n",
    "print(\"Original Tensor:\")\n",
    "print(original_tensor)\n",
    "\n",
    "print(\"\\nRepeated Tensor:\")\n",
    "print(repeated_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRIZ",
   "language": "python",
   "name": "triz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
