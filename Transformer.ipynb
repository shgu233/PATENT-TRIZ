{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_epochs = 500\n",
    "model_no = 'transformer_with_word2vec_without_zeros_and_labels'\n",
    "exp = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=100, nheads=5, num_encoder_layers=8, num_decoder_layers=8, avg_words = 5500):\n",
    "        super(custom_transformer, self).__init__()\n",
    "#         self.transformer = nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers, \n",
    "#                                           batch_first=True, activation=\"relu\")\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder, num_layers = num_encoder_layers)\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nheads, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder, num_layers = num_decoder_layers)\n",
    "        \n",
    "#         self.linear1 = nn.Linear(hidden_dim*avg_words, hidden_dim*avg_words, bias=True)\n",
    "#         self.one_D_conv = nn.Conv2d(hidden_dim*avg_words, hidden_dim*avg_words, 1, stride=1)\n",
    "#         self.relu_layer = nn.ReLU()\n",
    "#         self.sigmoid_layer = nn.Sigmoid()\n",
    "        \n",
    "    def positionalencoding1d(self, d_model, length):\n",
    "        \"\"\"\n",
    "        :param d_model: dimension of the model\n",
    "        :param length: length of positions\n",
    "        :return: length*d_model position matrix\n",
    "        \"\"\"\n",
    "        if d_model % 2 != 0:\n",
    "            raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                             \"odd dim (got dim={:d})\".format(d_model))\n",
    "        pe = torch.zeros(length, d_model)\n",
    "        position = torch.arange(0, length).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "                             -(math.log(10000.0) / d_model)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "\n",
    "        return pe\n",
    "\n",
    "    def forward(self, feat_input):\n",
    "#         print(feat_input.shape)\n",
    "#         feat_input = feat_input.flatten(2)\n",
    "#         print(feat_input.shape)\n",
    "#         print(self.positionalencoding1d(self.hidden_dim, feat_input.shape[-2]).repeat(feat_input.shape[0], feat_input.shape[1], feat_input.shape[2]).is_cuda)\n",
    "#         feat_input += self.positionalencoding1d(self.hidden_dim, feat_input.shape[-2]).repeat(feat_input.shape[0], feat_input.shape[1], feat_input.shape[2])\n",
    "#         features = self.transformer(feat_input.cuda(), self.learnable_query.repeat(feat_input.shape[0], 1, 1))\n",
    "        enc_features = self.transformer_encoder(feat_input)\n",
    "            \n",
    "        dec_features = self.transformer_decoder(feat_input, enc_features)\n",
    "\n",
    "#         features = self.linear1(dec_features.flatten(1))\n",
    "#         dec_features = self.sigmoid_layer(features)\n",
    "\n",
    "        return dec_features, enc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params: 8511264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhijeet/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "new_model = custom_transformer()\n",
    "# new_model.to(device)\n",
    "print(\"Total trainable params:\", torch.nn.utils.parameters_to_vector([p for p in new_model.parameters() if p.requires_grad]).numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_transformer(\n",
       "  (encoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_text = open('/home/abhijeet/Desktop/TRIZ/word_vectors_all_words_v2.json', 'r')\n",
    "dictionary_text = json.load(file_text)\n",
    "# file_label = open('/home/abhijeet/Desktop/TRIZ/labels.json', 'r')\n",
    "# dictionary_label = json.load(file_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader with zeros being appended\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class dataset_loader(Dataset):\n",
    "    \n",
    "#     def __init__(self, corpus_dir, word2vec_text, word2vec_label):\n",
    "#         self.word2vec_all_text = word2vec_text\n",
    "#         self.word2vec_all_label = word2vec_label\n",
    "#         self.dataset = pd.read_excel(corpus_dir + 'generated_data.xlsx')\n",
    "#         self.dataset = self.dataset.dropna()\n",
    "#         self.cols = self.dataset.columns\n",
    "#         self.dataset = self.dataset.values\n",
    "#         self.dataset = pd.DataFrame(self.dataset, columns = cols)\n",
    "#         self.text = self.dataset['text']\n",
    "#         self.label = self.dataset['labels']\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.text.shape[0]\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         row_text = self.text[idx].lower().replace('\\n', ' ')\n",
    "#         row_label = eval(self.label[idx])\n",
    "#         stop_words = set(stopwords.words('english'))\n",
    "#         word_tokens = word_tokenize(row_text)\n",
    "#         filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "#         filtered_sentence = []\n",
    "        \n",
    "#         for w in word_tokens:\n",
    "#             if w not in stop_words:\n",
    "#                 filtered_sentence.append(w)\n",
    "        \n",
    "#         row_text = filtered_sentence\n",
    "        \n",
    "# #         print(row_label, type(row_label))\n",
    "#         word2vec_matrix = []\n",
    "# #         print(len(row_text))\n",
    "#         for count, i in enumerate(row_text):\n",
    "#             try:\n",
    "#                 if count == 5000:\n",
    "#                     break\n",
    "#                 word2vec_matrix.append(np.array(self.word2vec_all_text[i]))\n",
    "# #                 print('Doing Text', count)\n",
    "#             except:\n",
    "#                 if count == 5000:\n",
    "#                     break\n",
    "#                 word2vec_matrix.append(np.zeros(100))\n",
    "        \n",
    "#         for i in range(5000 - len(word2vec_matrix)):\n",
    "#             word2vec_matrix.append(np.zeros(100))\n",
    "            \n",
    "# #         print(np.array(word2vec_matrix).shape)\n",
    "\n",
    "#         for next_count, i in enumerate(row_label):\n",
    "#             try:\n",
    "#                 if next_count == 20:\n",
    "#                     break\n",
    "#                 word2vec_matrix.append(np.array(self.word2vec_all_label[i]))\n",
    "# #                 print('Doing Labels')\n",
    "#             except:\n",
    "#                 if next_count == 20:\n",
    "#                     break\n",
    "#                 word2vec_matrix.append(np.zeros(100))\n",
    "            \n",
    "#         for i in range(5020 - len(word2vec_matrix)):\n",
    "#             word2vec_matrix.append(np.zeros(100))\n",
    "\n",
    "# #         print(np.array(word2vec_matrix).shape)\n",
    "        \n",
    "#         output = {'text_label': np.array(word2vec_matrix)}\n",
    "        \n",
    "#         if np.array(word2vec_matrix).shape != (5020, 100):\n",
    "#             print(np.array(word2vec_matrix).shape)\n",
    "#             output = {'text_label': np.array(word2vec_matrix[:5020])}\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class dataset_loader(Dataset):\n",
    "    \n",
    "    def __init__(self, corpus_dir, word2vec_text):\n",
    "        self.word2vec_all_text = word2vec_text\n",
    "#         self.word2vec_all_label = word2vec_label\n",
    "        self.dataset = pd.read_excel(corpus_dir + 'generated_data.xlsx')\n",
    "        self.dataset = self.dataset.dropna()\n",
    "        self.cols = self.dataset.columns\n",
    "        self.dataset = self.dataset.values\n",
    "        self.dataset = pd.DataFrame(self.dataset, columns = self.cols)\n",
    "        self.text = self.dataset['text']\n",
    "        self.label = self.dataset['labels']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.text.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row_text = self.text[idx].lower().replace('\\n', ' ')\n",
    "        row_label = eval(self.label[idx])\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(row_text)\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        filtered_sentence = []\n",
    "        \n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        \n",
    "        row_text = filtered_sentence\n",
    "        \n",
    "#         print(row_label, type(row_label))\n",
    "        word2vec_matrix = []\n",
    "#         print(len(row_text))\n",
    "        for count, i in enumerate(row_text):\n",
    "            try:\n",
    "                word2vec_matrix.append(np.array(self.word2vec_all_text[i]))\n",
    "#                 print('Doing Text', count)\n",
    "            except:\n",
    "                word2vec_matrix.append(np.zeros(100))\n",
    "\n",
    "#         print(np.array(word2vec_matrix).shape)\n",
    "        \n",
    "        output = {'text_label': np.array(word2vec_matrix)}\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text.close()\n",
    "# file_label.close()\n",
    "train_data = dataset_loader(corpus_dir = '/home/abhijeet/Desktop/TRIZ/All_data/CPC Data/',\n",
    "                              word2vec_text = dictionary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(train_data))\n",
    "for i in range(len(train_data)):\n",
    "    sample = train_data[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['text_label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "\n",
    "dataloader_train = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "# def criterion(predicted, target):\n",
    "#     \"\"\"\n",
    "#     Compute the Kullback-Leibler Divergence loss between two probability distributions.\n",
    "\n",
    "#     Args:\n",
    "#         p (torch.Tensor): True distribution (e.g., ground truth probabilities).\n",
    "#         q (torch.Tensor): Approximate distribution (e.g., predicted probabilities).\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: KL Divergence loss.\n",
    "#     \"\"\"\n",
    "#     assert target.shape == predicted.shape, \"Target and predicted tensors must have the same shape\"\n",
    "\n",
    "#     # Apply log softmax to the target and softmax to the predicted\n",
    "#     log_target = F.log_softmax(target, dim=1)\n",
    "#     softmax_predicted = F.softmax(predicted, dim=1)\n",
    "\n",
    "#     # Compute the KL divergence loss\n",
    "#     loss = F.kl_div(log_target, softmax_predicted, reduction='batchmean')\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def criterion(predicted, target):\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    return loss(predicted, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.to(device)\n",
    "optimizer = torch.optim.SGD(new_model.parameters(), lr=0.009, weight_decay=0.0001, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(new_model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a35b5f0a7ab480a91b7cfa2ad6871a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 1 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4507/2807553944.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_ground_truth = torch.tensor(features, requires_grad = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.036992594599723816 Validation Loss: 0.0 Epoch loss: 0.06882871317364987\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b87cab89a4049228d77f04ff3d5275d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 2 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.044040191918611526 Validation Loss: 0.0 Epoch loss: 0.048137393296319594\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33e3f16bb2c44cfb0526bc4cba6fa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 3 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.03540729731321335 Validation Loss: 0.0 Epoch loss: 0.0480715213587817274\n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72abdfb88c6d485d8912a812384c347f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 4 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.07038702070713043 Validation Loss: 0.0 Epoch loss: 0.0485237518433015725\n",
      "Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e675147878403c8ffd70da2a3ddb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 5 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.04217540845274925 Validation Loss: 0.0 Epoch loss: 0.0479759773343073146\n",
      "Epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d704725a644d47b3fa2801f1170376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 6 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.04777274653315544 Validation Loss: 0.0 Epoch loss: 0.0474771453906048456\n",
      "Epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab081182c8a449d19a19d2badd88d0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 7 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.04411599040031433 Validation Loss: 0.0 Epoch loss: 0.0472599953142836114\n",
      "Epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c1da2f7cfd4c7bac758d356dce2720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 8 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.04915597662329674 Validation Loss: 0.0 Epoch loss: 0.0472531153765073864\n",
      "Epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cb1c52a08246a79ca11faf85bb2703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 9 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.056823063641786575 Validation Loss: 0.0 Epoch loss: 0.046895185254823785\n",
      "Epoch: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2966c45ddb5e4cb6b4c0c27ef4075ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 10 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.0583166778087616 Validation Loss: 0.0 Epoch loss: 0.04689517702678938064\n",
      "Epoch: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc8426a353d41c7a39c09adae7739b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 11 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.04548110067844391 Validation Loss: 0.0 Epoch loss: 0.0465053267400597154\n",
      "Epoch: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ac421dbfac456eb66d9a0f9ba34c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 12 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.054987385869026184 Validation Loss: 0.0 Epoch loss: 0.046629328164444206\n",
      "Epoch: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74bac32cb17459cad8f661782432c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 13 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.03819659352302551 Validation Loss: 0.0 Epoch loss: 0.0463297336351967906\n",
      "Epoch: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06282baeb994f1d9e9fbda98aa024ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 14 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 173749 Training Loss: 0.03596973046660423 Validation Loss: 0.0 Epoch loss: 0.0460804376615958365\n",
      "Epoch: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4154c31eec4d079c1f1f0175f07790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 15 Step:   0%|          | 0/173749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 36416 Training Loss: 0.033448897302150726 Validation Loss: 0.0 Epoch loss: 0.046266531179214995"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     inner_pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader_train), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader_train, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      9\u001b[0m         features \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m         features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36mdataset_loader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m row_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel[idx])\n\u001b[1;32m     22\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 23\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m filtered_sentence \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     25\u001b[0m filtered_sentence \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/nltk/tokenize/destructive.py:160\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    157\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNCTUATION:\n\u001b[0;32m--> 160\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Handles parentheses.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m regexp, substitution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPARENS_BRACKETS\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/re.py:325\u001b[0m, in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    322\u001b[0m     template \u001b[38;5;241m=\u001b[39m sre_parse\u001b[38;5;241m.\u001b[39mparse_template(template, pattern)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sre_parse\u001b[38;5;241m.\u001b[39mexpand_template(template, match)\n\u001b[0;32m--> 325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_subx\u001b[39m(pattern, template):\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# internal: Pattern.sub/subn implementation helper\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     template \u001b[38;5;241m=\u001b[39m _compile_repl(template, pattern)\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m template[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(template[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;66;03m# literal replacement\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    num_nans = 0\n",
    "    running_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    print(f\"\\nEpoch: {epoch+1}\")\n",
    "    inner_pbar = tqdm(total=len(dataloader_train), position=1, leave=False, ascii=True, desc=f\"Epoch: {epoch+1} Step\")\n",
    "#     try:\n",
    "    for i, data in enumerate(dataloader_train, 1):\n",
    "        features = data['text_label']\n",
    "        features = features.type(torch.FloatTensor)\n",
    "        features = features.to(device)\n",
    "#         print(features.is_cuda)\n",
    "#         sigmoid = nn.Sigmoid()\n",
    "        y_ground_truth = torch.tensor(features, requires_grad = True)\n",
    "#         print(features.shape, y_ground_truth.shape)\n",
    "        optimizer.zero_grad()\n",
    "        preds, enc_results = new_model(features)\n",
    "\n",
    "        loss = criterion(predicted=preds, target=y_ground_truth)\n",
    "\n",
    "        inner_pbar.update(1)\n",
    "\n",
    "        running_loss += loss.detach().item()\n",
    "\n",
    "        print(f\"\\rStep: {i} Training Loss: {loss.item()} Validation Loss: {val_loss} Epoch loss: {running_loss/(i)}\", end=\"\")\n",
    "        # print(f\"\\rStep: {i} Training Loss: {loss.item()} Validation Loss: {val_loss} Nans: {num_nans}\")\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            if num_nans > 10:\n",
    "                raise RuntimeError(f\"Model Error: Encountered {num_nans} nan loss\")\n",
    "            num_nans += 1\n",
    "            continue\n",
    "        # loss.requires_grad = True\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i % 500 == 0):\n",
    "            if os.path.exists(f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/'):\n",
    "                pass\n",
    "            else:\n",
    "                os.makedirs(f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/exp_{str(exp)}/')\n",
    "\n",
    "            checkpoint_pth = f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/exp_{str(exp)}/trained_model_latest_epoch.pth'\n",
    "            torch.save(new_model.state_dict(), checkpoint_pth)\n",
    "\n",
    "    scheduler.step()\n",
    "    inner_pbar.close()\n",
    "#     except:\n",
    "#         if os.path.exists(f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/'):\n",
    "#             pass\n",
    "#         else:\n",
    "#             os.makedirs(f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/exp_{str(exp)}/')\n",
    "\n",
    "#         checkpoint_pth = f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/exp_{str(exp)}/trained_model_latest_epoch.pth'\n",
    "#         torch.save(new_model.state_dict(), checkpoint_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/'):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/exp_{str(exp)}/')\n",
    "\n",
    "checkpoint_pth = f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/exp_{str(exp)}/trained_model_latest_epoch.pth'\n",
    "torch.save(new_model.state_dict(), checkpoint_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRIZ",
   "language": "python",
   "name": "triz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
