{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 11\n",
    "n_epochs = 500\n",
    "model_no = 'transformer_with_word2vec'\n",
    "exp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=100, nheads=5, num_encoder_layers=8, num_decoder_layers=8, avg_words = 5500):\n",
    "        super(custom_transformer, self).__init__()\n",
    "#         self.transformer = nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers, \n",
    "#                                           batch_first=True, activation=\"relu\")\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder, num_layers = num_encoder_layers)\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nheads, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder, num_layers = num_decoder_layers)\n",
    "        \n",
    "#         self.linear1 = nn.Linear(hidden_dim*avg_words, hidden_dim*avg_words, bias=True)\n",
    "#         self.one_D_conv = nn.Conv2d(hidden_dim*avg_words, hidden_dim*avg_words, 1, stride=1)\n",
    "        self.relu_layer = nn.ReLU()\n",
    "        self.sigmoid_layer = nn.Sigmoid()\n",
    "        \n",
    "    def positionalencoding1d(self, d_model, length):\n",
    "        \"\"\"\n",
    "        :param d_model: dimension of the model\n",
    "        :param length: length of positions\n",
    "        :return: length*d_model position matrix\n",
    "        \"\"\"\n",
    "        if d_model % 2 != 0:\n",
    "            raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                             \"odd dim (got dim={:d})\".format(d_model))\n",
    "        pe = torch.zeros(length, d_model)\n",
    "        position = torch.arange(0, length).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "                             -(math.log(10000.0) / d_model)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "\n",
    "        return pe\n",
    "\n",
    "    def forward(self, feat_input):\n",
    "#         print(feat_input.shape)\n",
    "#         feat_input = feat_input.flatten(2)\n",
    "#         print(feat_input.shape)\n",
    "#         print(self.positionalencoding1d(self.hidden_dim, feat_input.shape[-2]).repeat(feat_input.shape[0], feat_input.shape[1], feat_input.shape[2]).is_cuda)\n",
    "#         feat_input += self.positionalencoding1d(self.hidden_dim, feat_input.shape[-2]).repeat(feat_input.shape[0], feat_input.shape[1], feat_input.shape[2])\n",
    "#         features = self.transformer(feat_input.cuda(), self.learnable_query.repeat(feat_input.shape[0], 1, 1))\n",
    "        enc_features = self.transformer_encoder(feat_input)\n",
    "            \n",
    "        dec_features = self.transformer_decoder(feat_input, enc_features)\n",
    "\n",
    "#         features = self.linear1(dec_features.flatten(1))\n",
    "#         dec_features = self.sigmoid_layer(features)\n",
    "\n",
    "        return dec_features, enc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params: 8511264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhijeet/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "new_model = custom_transformer()\n",
    "# new_model.to(device)\n",
    "print(\"Total trainable params:\", torch.nn.utils.parameters_to_vector([p for p in new_model.parameters() if p.requires_grad]).numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_transformer(\n",
       "  (encoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (relu_layer): ReLU()\n",
       "  (sigmoid_layer): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_text = open('/home/abhijeet/Desktop/TRIZ/word_vectors_all_words_v2.json', 'r')\n",
    "dictionary_text = json.load(file_text)\n",
    "file_label = open('/home/abhijeet/Desktop/TRIZ/labels.json', 'r')\n",
    "dictionary_label = json.load(file_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class dataset_loader(Dataset):\n",
    "    \n",
    "    def __init__(self, corpus_dir, word2vec_text, word2vec_label):\n",
    "        self.word2vec_all_text = word2vec_text\n",
    "        self.word2vec_all_label = word2vec_label\n",
    "        self.dataset = pd.read_excel(corpus_dir + 'generated_data.xlsx')\n",
    "        self.dataset = self.dataset.dropna()\n",
    "        self.text = self.dataset['text']\n",
    "        self.label = self.dataset['labels']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.text.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row_text = self.text[idx].lower().replace('\\n', ' ')\n",
    "        row_label = eval(self.label[idx])\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(row_text)\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        filtered_sentence = []\n",
    "        \n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        \n",
    "        row_text = filtered_sentence\n",
    "        \n",
    "#         print(row_label, type(row_label))\n",
    "        word2vec_matrix = []\n",
    "#         print(len(row_text))\n",
    "        for count, i in enumerate(row_text):\n",
    "            try:\n",
    "                if count == 5000:\n",
    "                    break\n",
    "                word2vec_matrix.append(np.array(self.word2vec_all_text[i]))\n",
    "#                 print('Doing Text', count)\n",
    "            except:\n",
    "                if count == 5000:\n",
    "                    break\n",
    "                word2vec_matrix.append(np.zeros(100))\n",
    "        \n",
    "        for i in range(5000 - len(word2vec_matrix)):\n",
    "            word2vec_matrix.append(np.zeros(100))\n",
    "            \n",
    "#         print(np.array(word2vec_matrix).shape)\n",
    "\n",
    "        for next_count, i in enumerate(row_label):\n",
    "            try:\n",
    "                if next_count == 20:\n",
    "                    break\n",
    "                word2vec_matrix.append(np.array(self.word2vec_all_label[i]))\n",
    "#                 print('Doing Labels')\n",
    "            except:\n",
    "                if next_count == 20:\n",
    "                    break\n",
    "                word2vec_matrix.append(np.zeros(100))\n",
    "            \n",
    "        for i in range(5020 - len(word2vec_matrix)):\n",
    "            word2vec_matrix.append(np.zeros(100))\n",
    "\n",
    "#         print(np.array(word2vec_matrix).shape)\n",
    "        \n",
    "        output = {'text_label': np.array(word2vec_matrix)}\n",
    "        \n",
    "        if np.array(word2vec_matrix).shape != (5020, 100):\n",
    "            print(np.array(word2vec_matrix).shape)\n",
    "            output = {'text_label': np.array(word2vec_matrix[:5020])}\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text.close()\n",
    "file_label.close()\n",
    "train_data = dataset_loader(corpus_dir = '/home/abhijeet/Desktop/TRIZ/All_data/CPC Data/',\n",
    "                              word2vec_text = dictionary_text, word2vec_label = dictionary_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(train_data))\n",
    "for i in range(len(train_data)):\n",
    "    sample = train_data[100]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5020, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['text_label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "\n",
    "dataloader_train = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "# def criterion(predicted, target):\n",
    "#     \"\"\"\n",
    "#     Compute the Kullback-Leibler Divergence loss between two probability distributions.\n",
    "\n",
    "#     Args:\n",
    "#         p (torch.Tensor): True distribution (e.g., ground truth probabilities).\n",
    "#         q (torch.Tensor): Approximate distribution (e.g., predicted probabilities).\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: KL Divergence loss.\n",
    "#     \"\"\"\n",
    "#     assert target.shape == predicted.shape, \"Target and predicted tensors must have the same shape\"\n",
    "\n",
    "#     # Apply log softmax to the target and softmax to the predicted\n",
    "#     log_target = F.log_softmax(target, dim=1)\n",
    "#     softmax_predicted = F.softmax(predicted, dim=1)\n",
    "\n",
    "#     # Compute the KL divergence loss\n",
    "#     loss = F.kl_div(log_target, softmax_predicted, reduction='batchmean')\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def criterion(predicted, target):\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    return loss(predicted, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.to(device)\n",
    "optimizer = torch.optim.SGD(new_model.parameters(), lr=0.009, weight_decay=0.0001, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(new_model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0176358ceaf4454fb7007e3e455c3575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 1 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_163971/2888089528.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_ground_truth = torch.tensor(features, requires_grad = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 527 Training Loss: 0.2466750293970108 Validation Loss: 0.0 Epoch loss: 0.407568389991655335\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779d549f405f474b973f17d748d5dcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 2 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 841 Training Loss: 0.25146180391311646 Validation Loss: 0.0 Epoch loss: 0.23051703840883966\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e9d166b22c4e4085cfe8aa95d02190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 3 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 152 Training Loss: 0.2774791717529297 Validation Loss: 0.0 Epoch loss: 0.202629222436562984\n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fee3d0b3d1470e966f96253b2e041a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 4 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 295 Training Loss: 0.2382313460111618 Validation Loss: 0.0 Epoch loss: 0.195396736163204028\n",
      "Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49fd99bf2334fbc8c3c1b3f4f59090c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 5 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 652 Training Loss: 0.13979338109493256 Validation Loss: 0.0 Epoch loss: 0.18021894630296098\n",
      "Epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fc2b53807144dc8b570df169cbee30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 6 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 273 Training Loss: 0.15239901840686798 Validation Loss: 0.0 Epoch loss: 0.17033692700413122\n",
      "Epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3650d998eb7e434e85b2b247898c3097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 7 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 785 Training Loss: 0.1408669650554657 Validation Loss: 0.0 Epoch loss: 0.151304923785719942\n",
      "Epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df04658c00549d3a71c26bbebacb165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 8 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 124 Training Loss: 0.17455480992794037 Validation Loss: 0.0 Epoch loss: 0.14234339415786739\n",
      "Epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271ee9afe3774e678c52111cb41e00d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 9 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 191 Training Loss: 0.11058580875396729 Validation Loss: 0.0 Epoch loss: 0.13880882338079484\n",
      "Epoch: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b0a05842194afd9a882efeda2ff5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 10 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1190 Training Loss: 0.08101800084114075 Validation Loss: 0.0 Epoch loss: 0.12655237497401845\n",
      "Epoch: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9825fca02b34003885da88e9b0088ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 11 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 400 Training Loss: 0.13064992427825928 Validation Loss: 0.0 Epoch loss: 0.114541444508358834\n",
      "Epoch: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abff64079fd4cc59331a65f572d1bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 12 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 71 Training Loss: 0.09306170791387558 Validation Loss: 0.0 Epoch loss: 0.10603960330637408\n",
      "Epoch: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913dee7cdc5b4f4bbe82ba345d156b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 13 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 287 Training Loss: 0.08499505370855331 Validation Loss: 0.0 Epoch loss: 0.107324082573116445\n",
      "Epoch: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899731da1b3e42e2a610975e807c4aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 14 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 543 Training Loss: 0.07517006248235703 Validation Loss: 0.0 Epoch loss: 0.10223509000809812\n",
      "Epoch: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366285b6eca74cdf956696e8a77ed3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 15 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 258 Training Loss: 0.12925468385219574 Validation Loss: 0.0 Epoch loss: 0.098674987796549655\n",
      "Epoch: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2076b21b0c7041809df323e1bfab1197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 16 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 129 Training Loss: 0.08404575288295746 Validation Loss: 0.0 Epoch loss: 0.09466298276832862\n",
      "Epoch: 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b18c191f1304306ae2238e3565cb923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 17 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 84 Training Loss: 0.10388702154159546 Validation Loss: 0.0 Epoch loss: 0.096370826519670947\n",
      "Epoch: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580d53c9ea0c48729bac68c89100035f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 18 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9 Training Loss: 0.09896181523799896 Validation Loss: 0.0 Epoch loss: 0.09448232584529453\n",
      "Epoch: 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bb67a5c65947b5b69abf37c7742e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 19 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 51 Training Loss: 0.0892123281955719 Validation Loss: 0.0 Epoch loss: 0.090524352794768775\n",
      "Epoch: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ebfdb282374eca896d8470f9ba28a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 20 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9 Training Loss: 0.07913972437381744 Validation Loss: 0.0 Epoch loss: 0.08847698900434706\n",
      "Epoch: 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c93a521d0f47be974f426e6777abac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 21 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 234 Training Loss: 0.08085363358259201 Validation Loss: 0.0 Epoch loss: 0.091637583863404077\n",
      "Epoch: 22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e62a3d5d054bb7b62b7fb799c7835e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 22 Step:   0%|          | 0/15796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 150 Training Loss: 0.10126844048500061 Validation Loss: 0.0 Epoch loss: 0.08878862845400969"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    num_nans = 0\n",
    "    running_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    print(f\"\\nEpoch: {epoch+1}\")\n",
    "    inner_pbar = tqdm(total=len(dataloader_train), position=1, leave=False, ascii=True, desc=f\"Epoch: {epoch+1} Step\")\n",
    "    try:\n",
    "        for i, data in enumerate(dataloader_train, 1):\n",
    "            features = data['text_label']\n",
    "            features = features.type(torch.FloatTensor)\n",
    "            features = features.to(device)\n",
    "    #         print(features.is_cuda)\n",
    "    #         sigmoid = nn.Sigmoid()\n",
    "            y_ground_truth = torch.tensor(features, requires_grad = True)\n",
    "    #         print(features.shape, y_ground_truth.shape)\n",
    "            optimizer.zero_grad()\n",
    "            preds, enc_results = new_model(features)\n",
    "\n",
    "            loss = criterion(predicted=preds, target=y_ground_truth)\n",
    "\n",
    "            inner_pbar.update(1)\n",
    "\n",
    "            running_loss += loss.detach().item()\n",
    "\n",
    "            print(f\"\\rStep: {i} Training Loss: {loss.item()} Validation Loss: {val_loss} Epoch loss: {running_loss/(i)}\", end=\"\")\n",
    "            # print(f\"\\rStep: {i} Training Loss: {loss.item()} Validation Loss: {val_loss} Nans: {num_nans}\")\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                if num_nans > 10:\n",
    "                    raise RuntimeError(f\"Model Error: Encountered {num_nans} nan loss\")\n",
    "                num_nans += 1\n",
    "                continue\n",
    "            # loss.requires_grad = True\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        if os.path.exists(f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/'):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/exp_{str(exp)}/')\n",
    "\n",
    "        checkpoint_pth = f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/exp_{str(exp)}/trained_model_latest_epoch.pth'\n",
    "        torch.save(new_model.state_dict(), checkpoint_pth)\n",
    "        \n",
    "        scheduler.step()\n",
    "        inner_pbar.close()\n",
    "    except:\n",
    "        if os.path.exists(f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/'):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/exp_{str(exp)}/')\n",
    "\n",
    "        checkpoint_pth = f'/home/abhijeet/Desktop/TRIZ/Model/model_{model_no}/exp_{str(exp)}/trained_model_latest_epoch.pth'\n",
    "        torch.save(new_model.state_dict(), checkpoint_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRIZ",
   "language": "python",
   "name": "triz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
