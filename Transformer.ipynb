{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhijeet/miniconda3/envs/TRIZ/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_epochs = 500\n",
    "model_no = 'transformer_with_word2vec'\n",
    "exp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 5500, 1])\n"
     ]
    }
   ],
   "source": [
    "# With square kernels and equal stride\n",
    "# m = nn.Conv2d(5500, 5500, 1, stride=1)\n",
    "# non-square kernels and unequal stride and with padding\n",
    "m = nn.Conv1d(5500, 5500, 1, stride=1)\n",
    "# # non-square kernels and unequal stride and with padding and dilation\n",
    "# m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
    "input = torch.randn(20, 5500, 1)\n",
    "output = m(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=100, nheads=5, num_encoder_layers=8, num_decoder_layers=8, avg_words = 5500):\n",
    "        super(custom_transformer, self).__init__()\n",
    "#         self.transformer = nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers, \n",
    "#                                           batch_first=True, activation=\"relu\")\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nheads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder, num_layers = num_encoder_layers)\n",
    "        \n",
    "        self.decoder = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nheads, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder, num_layers = num_decoder_layers)\n",
    "        \n",
    "#         self.linear1 = nn.Linear(hidden_dim*avg_words, hidden_dim*avg_words, bias=True)\n",
    "#         self.one_D_conv = nn.Conv2d(hidden_dim*avg_words, hidden_dim*avg_words, 1, stride=1)\n",
    "        self.relu_layer = nn.ReLU()\n",
    "        self.sigmoid_layer = nn.Sigmoid()\n",
    "        \n",
    "    def positionalencoding1d(self, d_model, length):\n",
    "        \"\"\"\n",
    "        :param d_model: dimension of the model\n",
    "        :param length: length of positions\n",
    "        :return: length*d_model position matrix\n",
    "        \"\"\"\n",
    "        if d_model % 2 != 0:\n",
    "            raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                             \"odd dim (got dim={:d})\".format(d_model))\n",
    "        pe = torch.zeros(length, d_model)\n",
    "        position = torch.arange(0, length).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "                             -(math.log(10000.0) / d_model)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "\n",
    "        return pe\n",
    "\n",
    "    def forward(self, feat_input):\n",
    "#         print(feat_input.shape)\n",
    "#         feat_input = feat_input.flatten(2)\n",
    "#         print(feat_input.shape)\n",
    "#         print(self.positionalencoding1d(self.hidden_dim, feat_input.shape[-2]).repeat(feat_input.shape[0], feat_input.shape[1], feat_input.shape[2]).is_cuda)\n",
    "#         feat_input += self.positionalencoding1d(self.hidden_dim, feat_input.shape[-2]).repeat(feat_input.shape[0], feat_input.shape[1], feat_input.shape[2])\n",
    "#         features = self.transformer(feat_input.cuda(), self.learnable_query.repeat(feat_input.shape[0], 1, 1))\n",
    "        enc_features = self.transformer_encoder(feat_input)\n",
    "            \n",
    "        dec_features = self.transformer_decoder(feat_input, enc_features)\n",
    "\n",
    "#         features = self.linear1(dec_features.flatten(1))\n",
    "        dec_features = self.sigmoid_layer(features)\n",
    "\n",
    "        return dec_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params: 8511264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhijeet/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "new_model = custom_transformer()\n",
    "# new_model.to(device)\n",
    "print(\"Total trainable params:\", torch.nn.utils.parameters_to_vector([p for p in new_model.parameters() if p.requires_grad]).numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "custom_transformer(\n",
       "  (encoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (relu_layer): ReLU()\n",
       "  (sigmoid_layer): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class dataset_loader(Dataset):\n",
    "    \n",
    "    def __init__(self, corpus_dir, word2vec_dir):\n",
    "        self.word2vec_all = open(word2vec_dir, 'r')\n",
    "        self.word2vec_all = json.load(self.word2vec_all)\n",
    "        self.dataset = pd.read_excel(corpus_dir + 'generated_data.xlsx')\n",
    "        self.dataset = self.dataset.dropna()\n",
    "        self.text = self.dataset['text']\n",
    "        self.label = self.dataset['labels']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.text.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row_text = self.text[idx].lower().replace('\\n', ' ').split()\n",
    "        row_label = eval(self.label[idx])\n",
    "#         print(row_label.shape, row_text.shape)\n",
    "        word2vec_matrix = []\n",
    "        \n",
    "        for count, i in enumerate(row_text):\n",
    "            try:\n",
    "                word2vec_matrix.append(np.array(self.word2vec_all[i]))\n",
    "                if count == 5000:\n",
    "                    break\n",
    "            except:\n",
    "                word2vec_matrix.append(np.zeros(100))\n",
    "        \n",
    "        for i in range(5000 - len(word2vec_matrix)):\n",
    "            word2vec_matrix.append(np.zeros(100))\n",
    "            \n",
    "#         print(len(word2vec_matrix))\n",
    "\n",
    "        for next_count, i in enumerate(row_label):\n",
    "            try:\n",
    "                word2vec_matrix.append(np.array(self.word2vec_all[i]))\n",
    "                if next_count == 499:\n",
    "                    break\n",
    "            except:\n",
    "                word2vec_matrix.append(np.zeros(100))\n",
    "            \n",
    "        for i in range(499 - next_count):\n",
    "            word2vec_matrix.append(np.zeros(100))\n",
    "\n",
    "#         print(len(word2vec_matrix))\n",
    "        \n",
    "        output = {'text_label': np.array(word2vec_matrix)}\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset_loader(corpus_dir='/home/abhijeet/Desktop/TRIZ/All_data/CPC Data/',\n",
    "                              word2vec_dir='/home/abhijeet/Desktop/TRIZ/word_vectors.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(train_data))\n",
    "for i in range(len(train_data)):\n",
    "    sample = train_data[i]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "\n",
    "dataloader_train = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def criterion(predicted, target):\n",
    "    \"\"\"\n",
    "    Compute the Kullback-Leibler Divergence loss between two probability distributions.\n",
    "\n",
    "    Args:\n",
    "        p (torch.Tensor): True distribution (e.g., ground truth probabilities).\n",
    "        q (torch.Tensor): Approximate distribution (e.g., predicted probabilities).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: KL Divergence loss.\n",
    "    \"\"\"\n",
    "    assert target.shape == predicted.shape, \"Target and predicted tensors must have the same shape\"\n",
    "\n",
    "    # Apply log softmax to the target and softmax to the predicted\n",
    "    log_target = F.log_softmax(target, dim=1)\n",
    "    softmax_predicted = F.softmax(predicted, dim=1)\n",
    "\n",
    "    # Compute the KL divergence loss\n",
    "    loss = F.kl_div(log_target, softmax_predicted, reduction='batchmean')\n",
    "\n",
    "    return loss\n",
    "#     return F.kl_div(predicted, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.to(device)\n",
    "optimizer = torch.optim.SGD(new_model.parameters(), lr=0.009, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                                 | 0/173749 [00:00<?, ?it/s]\u001b[A/tmp/ipykernel_9111/2194666015.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_ground_truth = torch.tensor(features, requires_grad = True)\n",
      "\n",
      "Epoch: 1 Step:   0%|                      | 1/173749 [00:00<11:39:35,  4.14it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                       | 2/173749 [00:00<8:04:37,  5.98it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 1 Training Loss: 2.1737499237060547 Validation Loss: 0.0 Epoch loss: 2.1737499237060547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                       | 3/173749 [00:00<7:29:08,  6.45it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                       | 4/173749 [00:00<7:15:51,  6.64it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 2 Training Loss: 0.6024421453475952 Validation Loss: 0.0 Epoch loss: 1.388096034526825\r",
      "Step: 3 Training Loss: 0.17988668382167816 Validation Loss: 0.0 Epoch loss: 0.9853595842917761"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                       | 5/173749 [00:00<7:07:25,  6.77it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                       | 6/173749 [00:00<7:00:01,  6.89it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 4 Training Loss: 0.854159951210022 Validation Loss: 0.0 Epoch loss: 0.9525596760213375\r",
      "Step: 5 Training Loss: 0.9188051223754883 Validation Loss: 0.0 Epoch loss: 0.9458087652921676"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                       | 7/173749 [00:01<7:00:46,  6.88it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                       | 8/173749 [00:01<6:49:55,  7.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 6 Training Loss: 0.5385774374008179 Validation Loss: 0.0 Epoch loss: 0.877936877310276\r",
      "Step: 7 Training Loss: 0.4847286343574524 Validation Loss: 0.0 Epoch loss: 0.8217642711741584"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                       | 9/173749 [00:01<6:54:49,  6.98it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 10/173749 [00:01<6:53:21,  7.01it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 8 Training Loss: 0.8768552541732788 Validation Loss: 0.0 Epoch loss: 0.8286506440490484\r",
      "Step: 9 Training Loss: 0.4892069697380066 Validation Loss: 0.0 Epoch loss: 0.7909346802367104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 11/173749 [00:01<6:49:07,  7.08it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 12/173749 [00:01<6:51:47,  7.03it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 10 Training Loss: 0.6125763654708862 Validation Loss: 0.0 Epoch loss: 0.773098848760128\r",
      "Step: 11 Training Loss: 0.2794008255004883 Validation Loss: 0.0 Epoch loss: 0.7282172102819789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 13/173749 [00:01<6:52:43,  7.02it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 14/173749 [00:02<6:49:57,  7.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 12 Training Loss: 1.097937822341919 Validation Loss: 0.0 Epoch loss: 0.759027261286974\r",
      "Step: 13 Training Loss: 0.5271157622337341 Validation Loss: 0.0 Epoch loss: 0.7411879152059555"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 15/173749 [00:02<6:59:14,  6.91it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 16/173749 [00:02<6:47:57,  7.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 14 Training Loss: 0.9379440546035767 Validation Loss: 0.0 Epoch loss: 0.7552419251629284\r",
      "Step: 15 Training Loss: 1.7827752828598022 Validation Loss: 0.0 Epoch loss: 0.8237441490093868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 17/173749 [00:02<6:41:06,  7.22it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 18/173749 [00:02<6:41:31,  7.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 16 Training Loss: 0.6799180507659912 Validation Loss: 0.0 Epoch loss: 0.8147550178691745\r",
      "Step: 17 Training Loss: 0.41428688168525696 Validation Loss: 0.0 Epoch loss: 0.7911980686818852"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 19/173749 [00:02<6:35:14,  7.33it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 20/173749 [00:02<6:35:34,  7.32it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 18 Training Loss: 0.4993324279785156 Validation Loss: 0.0 Epoch loss: 0.7749833108650314\r",
      "Step: 19 Training Loss: 0.7077677249908447 Validation Loss: 0.0 Epoch loss: 0.7714456484506005"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 21/173749 [00:03<6:40:32,  7.23it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 22/173749 [00:03<6:47:30,  7.11it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 20 Training Loss: 0.520099937915802 Validation Loss: 0.0 Epoch loss: 0.7588783629238606\r",
      "Step: 21 Training Loss: 0.3039865791797638 Validation Loss: 0.0 Epoch loss: 0.7372168494122369"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 23/173749 [00:03<6:46:45,  7.12it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 24/173749 [00:03<6:45:58,  7.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 22 Training Loss: 0.7507443428039551 Validation Loss: 0.0 Epoch loss: 0.7378317354754969\r",
      "Step: 23 Training Loss: 0.6483144760131836 Validation Loss: 0.0 Epoch loss: 0.7339396807162658"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 25/173749 [00:03<6:47:55,  7.10it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 26/173749 [00:03<6:49:48,  7.07it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 24 Training Loss: 0.38597607612609863 Validation Loss: 0.0 Epoch loss: 0.7194411971916755\r",
      "Step: 25 Training Loss: 0.47369274497032166 Validation Loss: 0.0 Epoch loss: 0.7096112591028213"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 27/173749 [00:03<7:05:49,  6.80it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 28/173749 [00:04<7:00:36,  6.88it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 26 Training Loss: 1.676978349685669 Validation Loss: 0.0 Epoch loss: 0.7468176856637001\r",
      "Step: 27 Training Loss: 0.9671545624732971 Validation Loss: 0.0 Epoch loss: 0.7549783107307222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 29/173749 [00:04<6:52:26,  7.02it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 30/173749 [00:04<6:55:42,  6.96it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 28 Training Loss: 0.28096190094947815 Validation Loss: 0.0 Epoch loss: 0.7380491532385349\r",
      "Step: 29 Training Loss: 1.2526551485061646 Validation Loss: 0.0 Epoch loss: 0.7557941875581083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 31/173749 [00:04<6:55:02,  6.98it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 32/173749 [00:04<6:52:31,  7.02it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 30 Training Loss: 0.6670126914978027 Validation Loss: 0.0 Epoch loss: 0.7528348043560982\r",
      "Step: 31 Training Loss: 0.235700324177742 Validation Loss: 0.0 Epoch loss: 0.7361530469309899"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 33/173749 [00:04<6:45:32,  7.14it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 34/173749 [00:04<6:56:53,  6.94it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 32 Training Loss: 0.5043062567710876 Validation Loss: 0.0 Epoch loss: 0.728907834738493\r",
      "Step: 33 Training Loss: 0.40166598558425903 Validation Loss: 0.0 Epoch loss: 0.7189914150671526"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 35/173749 [00:05<6:57:55,  6.93it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 36/173749 [00:05<6:50:28,  7.05it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 34 Training Loss: 0.13728468120098114 Validation Loss: 0.0 Epoch loss: 0.7018823934828534\r",
      "Step: 35 Training Loss: 0.8659298419952393 Validation Loss: 0.0 Epoch loss: 0.7065694634403501"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 37/173749 [00:05<6:57:58,  6.93it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 38/173749 [00:05<6:54:59,  6.98it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 36 Training Loss: 0.40056735277175903 Validation Loss: 0.0 Epoch loss: 0.698069404810667\r",
      "Step: 37 Training Loss: 0.8175098896026611 Validation Loss: 0.0 Epoch loss: 0.7012975260212615"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 39/173749 [00:05<6:52:14,  7.02it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 40/173749 [00:05<6:51:20,  7.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 38 Training Loss: 1.0123860836029053 Validation Loss: 0.0 Epoch loss: 0.7094840670102521\r",
      "Step: 39 Training Loss: 0.28905513882637024 Validation Loss: 0.0 Epoch loss: 0.6987038380824603"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 41/173749 [00:05<7:04:11,  6.82it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 42/173749 [00:06<7:07:12,  6.78it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 40 Training Loss: 2.0395355224609375 Validation Loss: 0.0 Epoch loss: 0.7322246301919222\r",
      "Step: 41 Training Loss: 0.42142221331596375 Validation Loss: 0.0 Epoch loss: 0.72464408343885"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 43/173749 [00:06<7:01:21,  6.87it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 44/173749 [00:06<6:53:50,  7.00it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 42 Training Loss: 1.5273233652114868 Validation Loss: 0.0 Epoch loss: 0.7437554949096271\r",
      "Step: 43 Training Loss: 0.643120288848877 Validation Loss: 0.0 Epoch loss: 0.7414151412803073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 45/173749 [00:06<7:04:11,  6.82it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 46/173749 [00:06<6:59:10,  6.91it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 44 Training Loss: 0.8368495106697083 Validation Loss: 0.0 Epoch loss: 0.7435841042209755\r",
      "Step: 45 Training Loss: 2.5515990257263184 Validation Loss: 0.0 Epoch loss: 0.7837622135877609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 47/173749 [00:06<7:08:03,  6.76it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 48/173749 [00:06<7:04:13,  6.82it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 46 Training Loss: 0.5309416651725769 Validation Loss: 0.0 Epoch loss: 0.77826611470917\r",
      "Step: 47 Training Loss: 0.8082621097564697 Validation Loss: 0.0 Epoch loss: 0.7789043273697508"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 49/173749 [00:07<6:53:25,  7.00it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 50/173749 [00:07<7:01:15,  6.87it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 48 Training Loss: 0.5133313536643982 Validation Loss: 0.0 Epoch loss: 0.7733715570842227\r",
      "Step: 49 Training Loss: 1.143277883529663 Validation Loss: 0.0 Epoch loss: 0.7809206657871908"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 51/173749 [00:07<7:09:53,  6.73it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 52/173749 [00:07<7:01:49,  6.86it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 50 Training Loss: 0.3951103091239929 Validation Loss: 0.0 Epoch loss: 0.7732044586539268\r",
      "Step: 51 Training Loss: 1.107986569404602 Validation Loss: 0.0 Epoch loss: 0.7797688137666852"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 53/173749 [00:07<6:52:42,  7.01it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 54/173749 [00:07<6:57:13,  6.94it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 52 Training Loss: 0.36590635776519775 Validation Loss: 0.0 Epoch loss: 0.7718099203820412\r",
      "Step: 53 Training Loss: 0.44168007373809814 Validation Loss: 0.0 Epoch loss: 0.7655810553510234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 55/173749 [00:07<6:58:55,  6.91it/s]\u001b[A\n",
      "Epoch: 1 Step:   0%|                      | 56/173749 [00:08<6:57:22,  6.94it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 54 Training Loss: 0.698840856552124 Validation Loss: 0.0 Epoch loss: 0.7643451257436363\r",
      "Step: 55 Training Loss: 0.955791175365448 Validation Loss: 0.0 Epoch loss: 0.7678259630094875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Step:   0%|                      | 57/173749 [00:08<6:50:20,  7.05it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step: 56 Training Loss: 0.9709116220474243 Validation Loss: 0.0 Epoch loss: 0.7714524926351649\r",
      "Step: 57 Training Loss: 0.48587048053741455 Validation Loss: 0.0 Epoch loss: 0.7664422818966079"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m y_ground_truth \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(features, requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mnew_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predicted\u001b[38;5;241m=\u001b[39mpreds, target\u001b[38;5;241m=\u001b[39my_ground_truth)\n\u001b[1;32m     19\u001b[0m inner_pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m, in \u001b[0;36mcustom_transformer.forward\u001b[0;34m(self, feat_input)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, feat_input):\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#         print(feat_input.shape)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#         feat_input = feat_input.flatten(2)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#         feat_input += self.positionalencoding1d(self.hidden_dim, feat_input.shape[-2]).repeat(feat_input.shape[0], feat_input.shape[1], feat_input.shape[2])\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#         features = self.transformer(feat_input.cuda(), self.learnable_query.repeat(feat_input.shape[0], 1, 1))\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m         enc_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m         dec_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_decoder(feat_input, enc_features)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#         features = self.linear1(dec_features.flatten(1))\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/transformer.py:391\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    388\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 391\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    394\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/transformer.py:714\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    712\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 714\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/transformer.py:722\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    721\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 722\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/activation.py:1243\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1242\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m-> 1243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m,\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1246\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1247\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1248\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1249\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1250\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1251\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/TRIZ/lib/python3.8/site-packages/torch/nn/modules/module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1677\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    num_nans = 0\n",
    "    running_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    print(f\"\\nEpoch: {epoch+1}\")\n",
    "    inner_pbar = tqdm(total=len(dataloader_train), position=1, leave=False, ascii=True, desc=f\"Epoch: {epoch+1} Step\")\n",
    "\n",
    "    for i, data in enumerate(dataloader_train, 1):\n",
    "        features = data['text_label']\n",
    "        features = features.type(torch.FloatTensor)\n",
    "        features = features.to(device)\n",
    "#         print(features.is_cuda)\n",
    "        y_ground_truth = torch.tensor(features, requires_grad = True)\n",
    "        optimizer.zero_grad()\n",
    "        preds = new_model(features)\n",
    "        \n",
    "        loss = criterion(predicted=preds, target=y_ground_truth)\n",
    "\n",
    "        inner_pbar.update(1)\n",
    "        \n",
    "        running_loss += loss.detach().item()\n",
    "\n",
    "        print(f\"\\rStep: {i} Training Loss: {loss.item()} Validation Loss: {val_loss} Epoch loss: {running_loss/(i)}\", end=\"\")\n",
    "        # print(f\"\\rStep: {i} Training Loss: {loss.item()} Validation Loss: {val_loss} Nans: {num_nans}\")\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            if num_nans > 10:\n",
    "                raise RuntimeError(f\"Model Error: Encountered {num_nans} nan loss\")\n",
    "            num_nans += 1\n",
    "            continue\n",
    "        # loss.requires_grad = True\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    inner_pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRIZ",
   "language": "python",
   "name": "triz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
